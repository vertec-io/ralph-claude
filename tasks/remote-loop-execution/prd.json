{
  "schemaVersion": "2.0",
  "project": "ralph-claude",
  "taskDir": "tasks/remote-loop-execution",
  "branchName": "ralph/remote-loop-execution",
  "mergeTarget": null,
  "autoMerge": false,
  "type": "bug-investigation",
  "description": "Remote Loop Execution - Dual-mode agent management (tmux for claude, opencode serve for opencode) with remote execution over OpenZiti",
  "userStories": [
    {
      "id": "US-001",
      "title": "Full architecture sketch",
      "description": "As a developer, I need a complete architectural document showing all components, their roles, communication paths, and how they compose into the remote execution system.",
      "acceptanceCriteria": [
        {
          "description": "Diagram showing: ralph-uv daemon, loop runners, RPC layer, OpenZiti overlay, client(s)",
          "passes": true
        },
        {
          "description": "Define the ralph-uv daemon's responsibilities (listen for start requests, manage loop lifecycles, expose per-loop RPC)",
          "passes": true
        },
        {
          "description": "Define how loop runners register with the daemon and expose their RPC",
          "passes": true
        },
        {
          "description": "Define client connection flow: client reads SQLite \u2192 if remote, connect via OpenZiti \u2192 remote daemon",
          "passes": true
        },
        {
          "description": "Define the 'start loop' request/response contract (origin URL, branch, task dir, iterations, agent)",
          "passes": true
        },
        {
          "description": "Define the remote bootstrapping flow: git push to remote bare repo \u2192 checkout \u2192 agent install check \u2192 task 0 (dep install) \u2192 loop start",
          "passes": true
        },
        {
          "description": "Define how multiple concurrent loops are isolated (separate checkouts, separate sockets, separate Ziti services or multiplexed?)",
          "passes": true
        },
        {
          "description": "Document which existing code is reused vs. what's new",
          "passes": true
        },
        {
          "description": "Address: what happens on disconnect/reconnect, daemon crash, loop crash",
          "passes": true
        },
        {
          "description": "Define loop completion flow: daemon pushes event \u2192 local SQLite marked completed/failed",
          "passes": true
        },
        {
          "description": "Address stale state: how does local SQLite reconcile if no client was connected when loop finished?",
          "passes": true
        },
        {
          "description": "Save architecture document to tasks/remote-loop-execution/architecture.md",
          "passes": true
        },
        {
          "description": "Typecheck passes",
          "passes": true
        }
      ],
      "priority": 1,
      "passes": true,
      "notes": "Architecture document saved to tasks/remote-loop-execution/architecture.md. Key decisions: (1) Proxy architecture - daemon proxies Ziti connections to loop runner Unix sockets. (2) Per-loop Ziti services for isolation. (3) Git worktrees for concurrent loop isolation. (4) Client pushes to origin, daemon fetches - avoids git-over-Ziti. (5) Task 0 handled by agent prompt, not special daemon logic. (6) Stale state resolved via active polling on status query + on-attach reconciliation. NOTE: Architecture has since shifted from RPC/PTY to tmux-based session management. Daemon now spawns loops in tmux sessions instead of managing PTY-based loop runners with RPC."
    },
    {
      "id": "US-002",
      "title": "Audit current RPC transport layer",
      "description": "As a developer, I need to understand exactly how the current Unix socket RPC is implemented so I can identify extension points for OpenZiti transport.",
      "acceptanceCriteria": [
        {
          "description": "Document all socket creation/binding code paths in rpc.py",
          "passes": true
        },
        {
          "description": "Document all client connection code in attach.py",
          "passes": true
        },
        {
          "description": "Identify where AF_UNIX is hardcoded vs. abstracted",
          "passes": true
        },
        {
          "description": "Map the full lifecycle: server start \u2192 client connect \u2192 subscribe \u2192 events \u2192 disconnect",
          "passes": true
        },
        {
          "description": "Note any assumptions that would break over a network (latency, ordering, disconnects)",
          "passes": true
        },
        {
          "description": "Update notes in prd.json with findings",
          "passes": true
        },
        {
          "description": "Typecheck passes",
          "passes": true
        }
      ],
      "priority": 2,
      "passes": true,
      "notes": "## RPC Transport Audit (SUPERSEDED by tmux architecture)\n\n**The RPC/PTY architecture documented here has been replaced with tmux-based session management.**\n\n### Current Architecture (tmux-based)\n- NO rpc.py, Unix sockets, or NDJSON event streaming\n- NO interactive.py or PTY management\n- Sessions run in detached tmux sessions via libtmux (session.py)\n- Attach is `tmux attach-session -t <name>` (attach.py) \u2014 direct terminal takeover\n- Stop/checkpoint uses signal files at ~/.local/share/ralph/signals/\n- Session state tracked in SQLite at ~/.local/share/ralph/sessions.db\n- tmux sessions use remain-on-exit so crash output is preserved\n- tmux_session_alive() checks pane_dead_status to distinguish alive vs dead-but-preserved\n\n### Remote Extension Points\n- The tmux server socket could be proxied over Ziti for remote attach\n- Signal files are local-only \u2014 remote stop/checkpoint needs a Ziti RPC call to daemon\n- SQLite is local-only \u2014 remote status needs daemon queries over Ziti\n- libtmux calls are local \u2014 daemon handles tmux lifecycle on the remote machine\n\n### Original Audit Summary (for reference, no longer applies)\n- AF_UNIX was hardcoded at rpc.py:150 (server) and attach.py:88 (client) \u2014 only 2 swap points\n- NDJSON framing was transport-agnostic (would have worked over any stream socket)\n- Interactive mode used PTY keystroke forwarding via write_pty()\n- RPC lifecycle was: server start \u2192 client connect \u2192 subscribe \u2192 events \u2192 disconnect"
    },
    {
      "id": "US-003",
      "title": "Evaluate OpenZiti Python SDK capabilities",
      "description": "As a developer, I need to understand the OpenZiti Python SDK's capabilities for both server (daemon) and client (attach/TUI) use.",
      "acceptanceCriteria": [
        {
          "description": "Document how to create a Ziti socket server (bind to a Ziti service) using openziti Python SDK",
          "passes": true
        },
        {
          "description": "Document how to connect as a Ziti client to a service using openziti Python SDK",
          "passes": true
        },
        {
          "description": "Determine if SDK supports asyncio (needed for RPC server integration)",
          "passes": true
        },
        {
          "description": "Determine how identity files (.json/.jwt) are loaded and used",
          "passes": true
        },
        {
          "description": "Test or document: can one SDK process host multiple services (one per loop)?",
          "passes": true
        },
        {
          "description": "Identify SDK limitations, maturity, and maintenance status",
          "passes": true
        },
        {
          "description": "Update notes in prd.json with findings",
          "passes": true
        },
        {
          "description": "Typecheck passes",
          "passes": true
        }
      ],
      "priority": 3,
      "passes": true,
      "notes": "## OpenZiti Python SDK Evaluation Findings (v1.5.0)\n\n### 1. Server-Side Socket Binding (Hosting a Ziti Service)\n\nThe SDK provides `ZitiContext.bind(service, terminator=None, sock=None)` for server-side hosting:\n\n```python\nimport openziti\n\n# Load identity and get context\nztx, err = openziti.load('/path/to/server-identity.json')\n\n# Bind to a Ziti service - returns a socket-like object\nserver = ztx.bind(service='ralph-loop-taskname-uuid')\nserver.listen(backlog=5)\n\n# Accept connections (blocking)\nconn, peer = server.accept()\ndata = conn.recv(8192)\nconn.sendall(response_data)\n```\n\n**Key mechanism**: The SDK wraps the C SDK (`libziti.so`) via ctypes. `Ziti_bind()` registers the socket with the Ziti network as a host for the named service. The returned socket uses a Ziti-managed file descriptor that supports standard socket ops (recv, send, accept).\n\n**Terminator support**: Multiple bindings to the same service with different terminators enables load balancing or routing to specific instances.\n\n### 2. Client-Side Connection\n\nTwo approaches:\n\n**Direct (ZitiContext.connect)**:\n```python\nztx, err = openziti.load('/path/to/client-identity.json')\nsock = ztx.connect('ralph-loop-taskname-uuid')  # by service name\n# sock is a standard socket.socket with Ziti-managed fd\nsock.sendall(request_data)\nresponse = sock.recv(8192)\n```\n\n**Via ZitiSocket (lower-level)**:\n```python\nimport openziti\nfrom socket import SOCK_STREAM\nsock = openziti.socket(type=SOCK_STREAM)\nsock.connect(('service-intercept-host', port))\n```\n\n**Monkey-patch approach** (for existing code, NOT suitable for our use case since we need selective Ziti vs local sockets).\n\n### 3. Asyncio Support\n\n**CRITICAL FINDING: No native asyncio support.** The SDK operates at the fd level using the C SDK's internal event loop.\n\n**On Linux**: The Ziti fds returned by `Ziti_socket()` ARE real file descriptors backed by the C SDK's internal socketpair mechanism. On Linux, they work with `select()`, `poll()`, and by extension, asyncio's `add_reader()`/`add_writer()` since the Linux asyncio event loop uses `selectors.EpollSelector` which monitors fds.\n\n**Evidence**: The SDK's own code uses `select.select()` in `RpcClient.read_events()` (attach.py:165-200) with socket fds. The `ZitiSocket` extends `socket.socket` passing the Ziti fd as `fileno`.\n\n**Implications for our architecture**:\n- The PROXY approach is strongly preferred: daemon accepts Ziti connections in a dedicated thread, bridges to Unix sockets that asyncio already handles\n- Alternatively, asyncio can monitor Ziti fds directly via `loop.add_reader(fd, callback)` on Linux (where Ziti fds are epoll-compatible)\n- Windows asyncio (ProactorEventLoop) is NOT compatible (confirmed by issue #92)\n- We only target Linux for the remote daemon, so this is acceptable\n\n**Recommended integration pattern**:\n```python\n# Daemon thread: accept Ziti connections, proxy to Unix sockets\ndef ziti_proxy_thread(ztx, service_name, unix_socket_path):\n    server = ztx.bind(service=service_name)\n    server.listen(5)\n    while True:\n        ziti_conn, peer = server.accept()  # blocking in this thread\n        # For each connection, spawn a proxy coroutine\n        asyncio.run_coroutine_threadsafe(\n            proxy_connection(ziti_conn, unix_socket_path),\n            main_loop\n        )\n```\n\n### 4. Identity File Loading\n\n**Loading**: `openziti.load(path, timeout=0)` -> returns `(ZitiContext, error_code)`\n- `path`: string path to a `.json` identity file (or directory containing identity)\n- `timeout`: milliseconds to wait for enrollment/auth (0 = default)\n- Identity files are JSON containing certs, keys, and controller URL\n- The file is read once; the C SDK maintains the connection to the Ziti controller\n\n**Enrollment**: For new identities, use `openziti.enroll(jwt_path)` with a `.jwt` token file. Returns JSON identity string to save to disk.\n\n**Environment variable**: `ZITI_IDENTITIES` env var (semicolon-separated paths) auto-loads identities at import time.\n\n**One context per identity**: Each `openziti.load()` call creates a separate `ZitiContext`. Multiple contexts can coexist in one process.\n\n**External auth (MFA/OIDC)**: v1.0+ supports `get_external_signers()`, `login_external()`, `login_totp()`, and `wait_for_auth()` for interactive auth flows.\n\n### 5. Multiple Services Per Process\n\n**YES - confirmed feasible.** The SDK supports hosting multiple services from one process:\n\n1. **Single identity, multiple binds**: One `ZitiContext` can call `.bind()` multiple times with different service names. Each returns an independent server socket.\n2. **Multiple identities**: Multiple `openziti.load()` calls create independent contexts, each capable of binding its own services.\n3. **Terminator-based routing**: Same service name with different terminators allows routing to specific loop runners.\n\n**For our architecture**:\n- Daemon loads ONE server identity\n- Calls `ztx.bind(service=f'ralph-loop-{task}-{uuid}')` for each active loop\n- Each bind returns an independent socket that can accept connections in its own thread\n- Control service (`ralph-control-{hostname}`) is a separate bind on same context\n\n**Isolation model**: Each bound service gets its own accept loop. Connections to service A cannot reach service B. This matches our per-loop Ziti service design.\n\n### 6. SDK Maturity and Maintenance Status\n\n**Status: Alpha (classified), but actively maintained**\n- PyPI classifier: `Development Status :: 3 - Alpha`\n- Latest release: v1.5.0 (Nov 18, 2025) - 2 months old, actively updated\n- Release cadence: ~monthly (27 releases since Apr 2022)\n- GitHub: 86 stars, 7 forks, 13 open issues\n- Maintainers: NetFoundry Inc (commercial backing via CloudZiti)\n- License: Apache 2.0\n- Architecture: Python ctypes wrapper around C SDK (`libziti` v1.9.16)\n- Platforms: Linux (primary), macOS, Windows (limited asyncio support)\n- Python version: 3.4+ (classifiers), practically 3.8+ for type hints used\n\n**Limitations identified**:\n1. **No native asyncio**: Blocking socket API only. Must use threads for server accept loops.\n2. **Windows asyncio broken**: Issue #92 open, ProactorEventLoop incompatible.\n3. **No backpressure**: send/recv are blocking; no built-in flow control.\n4. **No reconnection**: If controller connection drops, context must be recreated.\n5. **Source-only distribution**: No pre-built wheels; requires `libziti.so` bundled in package.\n6. **Thread safety unclear**: C SDK is thread-safe, but Python wrapper doesn't document thread safety of ZitiContext methods.\n7. **No async accept/connect**: All I/O is blocking; requires threading or manual fd monitoring.\n\n**Strengths**:\n1. Real fd-based sockets - compatible with select/poll/epoll on Linux\n2. Minimal API surface - very few functions to understand\n3. Transparent socket subclass - works with existing socket-expecting code\n4. Commercial backing ensures continued development\n5. Proven in production (FastAPI/uvicorn use cases documented)\n\n### 7. Implications for Ralph Architecture\n\n**Recommended approach (proxy pattern)**:\n- Daemon runs Ziti accept loops in dedicated threads (one per bound service)\n- Each accepted Ziti connection spawns a proxy that bridges to the loop runner's existing Unix socket\n- Loop runner code remains UNCHANGED (still uses asyncio.start_unix_server)\n- Client (attach.py) can use Ziti directly for connect() since it's synchronous already\n\n**Alternative approach (fd-based asyncio integration, higher risk)**:\n- Register Ziti server fd with asyncio via `loop.add_reader()`\n- Would avoid threads but requires careful handling of Ziti's internal event loop interactions\n- Not recommended due to undocumented fd behavior edge cases\n\n**Installation on remote**: `pip install openziti` (37KB source, pulls in libziti native lib). No additional system dependencies beyond Python 3.8+.\n\n### Impact of tmux migration\nThe asyncio limitation is LESS critical now. The previous architecture needed asyncio for the RPC event loop. Current architecture uses tmux sessions + signal files, so the daemon only needs Ziti for: (1) accepting start-loop requests, (2) proxying tmux sessions for remote attach, (3) responding to status queries. These can use blocking Ziti sockets in dedicated threads without asyncio integration."
    },
    {
      "id": "US-004",
      "title": "Implement opencode server mode for local loops",
      "description": "As a developer, I need ralph-uv to use opencode serve instead of tmux when the agent is opencode, so that attach uses the native opencode TUI.",
      "acceptanceCriteria": [
        {
          "description": "When agent=opencode, ralph-uv run starts opencode serve --port <auto> instead of tmux session",
          "passes": false
        },
        {
          "description": "Ralph-uv tracks the opencode server PID and port in SQLite session entry",
          "passes": false
        },
        {
          "description": "Loop runner sends prompts via HTTP API (POST /session/:id/message) instead of shell injection",
          "passes": false
        },
        {
          "description": "Loop runner monitors completion via SSE events (GET /event) \u2014 waits for session.idle event",
          "passes": false
        },
        {
          "description": "ralph-uv attach <task> launches opencode attach http://localhost:<port> for opencode loops",
          "passes": false
        },
        {
          "description": "ralph-uv stop <task> calls POST /session/:id/abort for opencode loops",
          "passes": false
        },
        {
          "description": "Health check on startup: verify GET /global/health returns before sending prompts",
          "passes": false
        },
        {
          "description": "Opencode server process managed with proper lifecycle (start, health check, kill on completion)",
          "passes": false
        },
        {
          "description": "Typecheck passes",
          "passes": false
        }
      ],
      "priority": 4,
      "passes": false,
      "notes": "Key opencode server APIs: POST /session (create), POST /session/:id/message (sync prompt), POST /session/:id/prompt_async (async prompt), GET /event (SSE), POST /session/:id/abort (stop), GET /global/health (liveness). Default port 4096, auto-increments if busy. Server stores sessions in ~/.opencode/data/storage/. OPENCODE_SERVER_PASSWORD for auth."
    },
    {
      "id": "US-005",
      "title": "Assess remote attach/monitoring over network",
      "description": "As a developer, I need to determine if remote session attach is feasible over OpenZiti for both agent types.",
      "acceptanceCriteria": [
        {
          "description": "Document remote opencode attach flow: Ziti proxies HTTP port -> opencode attach http://<ziti>:<port>",
          "passes": false
        },
        {
          "description": "Analyze remote claude attach options (tmux socket proxy vs SSH-over-Ziti)",
          "passes": false
        },
        {
          "description": "Assess latency requirements for remote terminal/TUI session to feel responsive",
          "passes": false
        },
        {
          "description": "Test or estimate: SSE event latency over Ziti for opencode real-time updates",
          "passes": false
        },
        {
          "description": "Document architecture for both agent types remote attach",
          "passes": false
        },
        {
          "description": "Update notes in prd.json with findings",
          "passes": false
        },
        {
          "description": "Typecheck passes",
          "passes": false
        }
      ],
      "priority": 5,
      "passes": false,
      "notes": "For opencode: remote attach is trivial \u2014 just proxy HTTP port over Ziti, then opencode attach <url>. For claude: tmux socket proxying is harder \u2014 may need SSH-over-Ziti. Key insight: opencode server mode makes remote much simpler than tmux proxying."
    },
    {
      "id": "US-006",
      "title": "Design unified session DB schema for dual-mode + remote",
      "description": "As a developer, I need to extend the SQLite session schema to support both agent types (tmux vs opencode-server) and remote loops.",
      "acceptanceCriteria": [
        {
          "description": "Design schema changes: add session_type (tmux|opencode-server), server_port, server_url, remote flag, Ziti service name",
          "passes": false
        },
        {
          "description": "Define how opencode-server sessions are registered (port, PID, health URL)",
          "passes": false
        },
        {
          "description": "Define how remote loops are registered in local SQLite (on start-remote command)",
          "passes": false
        },
        {
          "description": "Ensure ralph-uv status can list all loops (local tmux, local opencode, remote tmux, remote opencode)",
          "passes": false
        },
        {
          "description": "Define how stale entries are cleaned up per type (tmux: pane_dead_status, opencode: health check failure)",
          "passes": false
        },
        {
          "description": "Define how ralph-tui dispatches attach per type (tmux pipe-pane vs opencode attach)",
          "passes": false
        },
        {
          "description": "Update notes in prd.json with findings",
          "passes": false
        },
        {
          "description": "Typecheck passes",
          "passes": false
        }
      ],
      "priority": 6,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Design remote environment bootstrapping flow",
      "description": "As a developer, I need to understand how the remote machine gets the code, agent CLI, and project deps when a job is sent to it.",
      "acceptanceCriteria": [
        {
          "description": "Design code sync flow: local creates branch -> pushes to origin -> remote daemon fetches and checks out",
          "passes": false
        },
        {
          "description": "Design agent CLI auto-install: daemon checks for claude/opencode binary, installs if missing",
          "passes": false
        },
        {
          "description": "Document what install opencode CLI looks like (curl installer, bun dependency)",
          "passes": false
        },
        {
          "description": "Design task 0 pattern: first iteration installs project deps before real work",
          "passes": false
        },
        {
          "description": "Define manual one-time setup per remote (git, python, bun/node for opencode, API keys)",
          "passes": false
        },
        {
          "description": "Document full start-remote sequence from user command to loop iteration 1",
          "passes": false
        },
        {
          "description": "Update notes in prd.json with findings",
          "passes": false
        },
        {
          "description": "Typecheck passes",
          "passes": false
        }
      ],
      "priority": 7,
      "passes": false,
      "notes": "Key decisions from earlier: (1) Code arrives via git push/fetch. (2) Agent CLI auto-installed by daemon. (3) API keys pre-configured manually. (4) Project deps installed by agent as task 0. (5) For opencode: need bun/node on remote. (6) Remote daemon spawns opencode serve for opencode loops, tmux for claude loops."
    },
    {
      "id": "US-008",
      "title": "Prototype opencode remote loop over Ziti",
      "description": "As a developer, I need to validate the architecture with a proof-of-concept: start opencode serve on remote, proxy via Ziti, attach locally.",
      "acceptanceCriteria": [
        {
          "description": "Set up OpenZiti network (controller + edge router, or CloudZiti)",
          "passes": false
        },
        {
          "description": "Enroll a ralph-uv daemon server identity and a client identity",
          "passes": false
        },
        {
          "description": "Create a Ziti service that proxies to the remote opencode HTTP port",
          "passes": false
        },
        {
          "description": "Start opencode serve on remote, verify opencode attach works via Ziti proxy locally",
          "passes": false
        },
        {
          "description": "Send a prompt via Ziti-proxied HTTP API and verify response",
          "passes": false
        },
        {
          "description": "Verify SSE event streaming works over Ziti (real-time progress updates)",
          "passes": false
        },
        {
          "description": "Measure latency: local opencode attach vs Ziti-proxied opencode attach",
          "passes": false
        },
        {
          "description": "Test abort/stop via Ziti-proxied API",
          "passes": false
        },
        {
          "description": "Document any issues discovered during prototype",
          "passes": false
        },
        {
          "description": "Update notes in prd.json with findings and measurements",
          "passes": false
        },
        {
          "description": "Typecheck passes",
          "passes": false
        }
      ],
      "priority": 8,
      "passes": false,
      "notes": ""
    }
  ],
  "agent": "opencode"
}
